{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce46470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has a total of 5572 samples\n",
      "classes are: {'spam', 'ham'}\n",
      "training samples: 4458\n",
      "test samples: 1114\n",
      "accuracy: 0.9811\n",
      "precision: 0.9706\n",
      "recall: 0.8859\n",
      "F1-Score: 0.9263\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from math import log\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_priors = {}\n",
    "        self.feature_probs = {}\n",
    "        self.vocabulary = set()\n",
    "        self.classes = []\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        words = text.split()\n",
    "        return words\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = list(set(y))\n",
    "        n_samples = len(X)\n",
    "        class_counts = Counter(y)\n",
    "\n",
    "        for cls in self.classes:\n",
    "            self.class_priors[cls] = class_counts[cls] / n_samples\n",
    "\n",
    "        class_word_counts = defaultdict(lambda: defaultdict(int))\n",
    "        class_total_words = defaultdict(int)\n",
    "\n",
    "        for text, label in zip(X, y):\n",
    "            words = self.preprocess_text(text)\n",
    "            for word in words:\n",
    "                self.vocabulary.add(word)\n",
    "                class_word_counts[label][word] += 1\n",
    "                class_total_words[label] += 1\n",
    "\n",
    "        vocab_size = len(self.vocabulary)\n",
    "\n",
    "        for cls in self.classes:\n",
    "            self.feature_probs[cls] = {}\n",
    "            total_words = class_total_words[cls]\n",
    "            for word in self.vocabulary:\n",
    "                word_count = class_word_counts[cls][word]\n",
    "                self.feature_probs[cls][word] = (word_count + self.alpha) / (total_words + self.alpha * vocab_size)\n",
    "\n",
    "    def predict_single(self, text):\n",
    "        words = self.preprocess_text(text)\n",
    "        word_counts = Counter(words)\n",
    "        class_scores = {}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            score = log(self.class_priors[cls])\n",
    "            for word, count in word_counts.items():\n",
    "                if word in self.vocabulary:\n",
    "                    score += count * log(self.feature_probs[cls][word])\n",
    "            class_scores[cls] = score\n",
    "\n",
    "        return max(class_scores, key=class_scores.get)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self.predict_single(text) for text in X]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        probabilities = []\n",
    "\n",
    "        for text in X:\n",
    "            words = self.preprocess_text(text)\n",
    "            word_counts = Counter(words)\n",
    "            class_scores = {}\n",
    "\n",
    "            for cls in self.classes:\n",
    "                score = log(self.class_priors[cls])\n",
    "                for word, count in word_counts.items():\n",
    "                    if word in self.vocabulary:\n",
    "                        score += count * log(self.feature_probs[cls][word])\n",
    "                class_scores[cls] = score\n",
    "\n",
    "            max_score = max(class_scores.values())\n",
    "            exp_scores = {cls: np.exp(score - max_score) for cls, score in class_scores.items()}\n",
    "            total = sum(exp_scores.values())\n",
    "            probs = {cls: exp_scores[cls] / total for cls in self.classes}\n",
    "            probabilities.append(probs)\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "def train_test_split(X, y, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    n_samples = len(X)\n",
    "    n_test = int(n_samples * test_size)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    test_indices = indices[:n_test]\n",
    "    train_indices = indices[n_test:]\n",
    "    X_train = [X[i] for i in train_indices]\n",
    "    X_test = [X[i] for i in test_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n",
    "    return correct / len(y_true)\n",
    "\n",
    "def precision_recall_f1(y_true, y_pred, pos_label='spam'):\n",
    "    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == pos_label and pred == pos_label)\n",
    "    fp = sum(1 for true, pred in zip(y_true, y_pred) if true != pos_label and pred == pos_label)\n",
    "    fn = sum(1 for true, pred in zip(y_true, y_pred) if true == pos_label and pred != pos_label)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    df = pd.read_csv(file_path, encoding='latin-1')\n",
    "    if 'v1' in df.columns and 'v2' in df.columns:\n",
    "        df = df[['v1', 'v2']]\n",
    "        df.columns = ['label', 'text']\n",
    "    elif 'label' in df.columns and 'text' in df.columns:\n",
    "        df = df[['label', 'text']]\n",
    "    else:\n",
    "        df.columns = ['label', 'text'] + [f'col_{i}' for i in range(2, len(df.columns))]\n",
    "        df = df[['label', 'text']]\n",
    "    df = df.dropna()\n",
    "    return df['text'].tolist(), df['label'].tolist()\n",
    "\n",
    "def main():\n",
    "    file_path = 'spam.csv'\n",
    "    \n",
    "    X, y = load_and_preprocess_data(file_path)\n",
    "    \n",
    "    print(f\"dataset has a total of {len(X)} samples\")\n",
    "    print(f\"classes are: {set(y)}\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"training samples: {len(X_train)}\")\n",
    "    print(f\"test samples: {len(X_test)}\")\n",
    "    \n",
    "    nb_classifier = MultinomialNaiveBayes(alpha=1.0)\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1 = precision_recall_f1(y_test, y_pred, pos_label='spam')\n",
    "\n",
    "    print(f\"accuracy: {accuracy:.4f}\")\n",
    "    print(f\"precision: {precision:.4f}\")\n",
    "    print(f\"recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    return nb_classifier\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8b939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 'Congratulations! You just won a lottery!'\n",
      "prediction: spam\n",
      "probabilities: {'ham': 0.004531476796176658, 'spam': 0.9954685232038233}\n",
      "\n",
      "text: 'The results for the KU entrance exam will be published next week.'\n",
      "prediction: ham\n",
      "probabilities: {'ham': 0.9309508096046316, 'spam': 0.06904919039536837}\n",
      "\n",
      "text: 'URGENT: Your account will be closed'\n",
      "prediction: spam\n",
      "probabilities: {'ham': 0.11454321028037505, 'spam': 0.885456789719625}\n",
      "\n",
      "text: 'Get your Kathmandu University degree without attending classes!'\n",
      "prediction: ham\n",
      "probabilities: {'ham': 0.9669097708531139, 'spam': 0.033090229146886134}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    \"Congratulations! You just won a lottery!\",\n",
    "    \"The results for the KU entrance exam will be published next week.\",\n",
    "    \"URGENT: Your account will be closed\",\n",
    "    \"Get your Kathmandu University degree without attending classes!\",\n",
    "]\n",
    "\n",
    "for text in samples:\n",
    "    prediction = classifier.predict_single(text)\n",
    "    probabilities = classifier.predict_proba([text])[0]\n",
    "    print(f\"text: '{text}'\")\n",
    "    print(f\"prediction: {prediction}\")\n",
    "    print(f\"probabilities: {probabilities}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
